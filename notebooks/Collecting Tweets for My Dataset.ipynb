{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tweepy in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (3.9.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from tweepy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy) (1.25.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /opt/anaconda3/envs/data-x/lib/python3.7/site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tweepy\n",
    "\n",
    "import tweepy #https://github.com/tweepy/tweepy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Twitter API credentials\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "access_key = \"\"\n",
    "access_secret = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_acc = []\n",
    "final_t = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start = pd.to_datetime('1/1/20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name, users):\n",
    "    #Twitter only allows access to a users most recent 3240 tweets with this method\n",
    "    \n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "    # auth.set_access_token(access_key, access_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = [] \n",
    "        \n",
    "    initial = True\n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    while initial:\n",
    "        try:\n",
    "            new_tweets = api.user_timeline(screen_name = screen_name,count=200)\n",
    "            initial = False\n",
    "        except tweepy.TweepError:\n",
    "            print('PAUSE')\n",
    "            time.sleep(60*15)\n",
    "            continue\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "        \n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while len(alltweets) <= 3150 and alltweets[-1].created_at > year_start:\n",
    "        \n",
    "        try:\n",
    "            #all subsiquent requests use the max_id param to prevent duplicates\n",
    "            new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)\n",
    "            print(len(alltweets), end=' ')\n",
    "\n",
    "        except tweepy.TweepError:\n",
    "            print('PAUSE')\n",
    "            time.sleep(60 * 15)\n",
    "            continue\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "\n",
    "    \n",
    "    complete = True\n",
    "    while complete:\n",
    "        try:\n",
    "            user = api.get_user(screen_name)\n",
    "            complete = False\n",
    "        except tweepy.TweepError:\n",
    "            print('PAUSE')\n",
    "            time.sleep(60*15)\n",
    "            continue    \n",
    "    \n",
    "    users.append(user)\n",
    "        \n",
    "    print(screen_name, end=' ')\n",
    "    \n",
    "    return alltweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "canidates_Rep = ['realDonaldTrump', 'Mike_Pence', 'POTUS', 'VP']\n",
    "canidates_Dem = ['JoeBiden', 'KamalaHarris', 'SenKamalaHarris']\n",
    "politicians_Rep = ['DanCrenshawTX', 'SteveScalise', 'GOPLeader', 'senatemajldr', 'newtgingrich', 'SpeakerBoehner', 'MittRomney', 'tedcruz', 'SpeakerRyan', 'marcorubio']\n",
    "politicians_Dem = ['AOC', 'SpeakerPelosi', 'IlhanMN', 'CoryBooker', 'ewarren', 'BernieSanders', 'BarackObama', 'HillaryClinton', 'NYGovCuomo', 'PeteButtigieg']\n",
    "pundits_Rep = ['benshapiro', 'RubinReport', 'glennbeck', 'scrowder', 'seanhannity', 'TuckerCarlson', 'charliekirk11', 'RealCandaceO', 'DonaldJTrumpJr', 'RealRLimbaugh']\n",
    "pundits_Dem = ['maddow', 'Trevornoah', 'iamjohnoliver', 'StephenAtHome', 'sethmeyers', 'billmaher', 'VanJones68', 'shaunking', 'RBReich', 'iamsambee']\n",
    "organizations_Rep = ['WSJ', 'FoxNews', 'nypost', 'dcexaminer', 'OANN', 'WashTimes', 'reason', 'BreitbartNews', 'FDRLST', 'newsmax']\n",
    "organizations_Dem = ['TheAtlantic', 'business', 'CNNPolitics', 'MSNBC', 'guardian', 'nytimes', 'nprpolitics', 'washingtonpost', 'voxdotcom', 'BuzzFeedNews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_list=politicians_Rep+politicians_Dem+pundits_Rep+pundits_Dem+organizations_Rep+organizations_Dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 TheAtlantic 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 business 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 CNNPolitics 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 MSNBC 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 guardian 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 nytimes 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 nprpolitics 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 washingtonpost 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 voxdotcom 200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000 BuzzFeedNews "
     ]
    }
   ],
   "source": [
    "users = []\n",
    "tweets = []\n",
    "count = 0\n",
    "for account in total_list[50:]:\n",
    "    tweet = get_all_tweets(account, users)\n",
    "    tweets.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_acc += users\n",
    "final_t += tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(final_acc, open(\"accounts.p\", \"wb\" ))\n",
    "pickle.dump(final_t, open(\"tweets.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
